{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.11/site-packages (23.3.1)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.11/site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from datasets) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./venv/lib/python3.11/site-packages (from datasets) (14.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in ./venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in ./venv/lib/python3.11/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./venv/lib/python3.11/site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./venv/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.11/site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./venv/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in ./venv/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "zsh:1: no matches found: ray[tune]==2.6.3\n",
      "Requirement already satisfied: accelerate in ./venv/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./venv/lib/python3.11/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in ./venv/lib/python3.11/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install --upgrade datasets\n",
    "!pip3 install --upgrade scikit-learn\n",
    "!pip3 install --upgrade torch\n",
    "!pip3 install --upgrade transformers\n",
    "!pip3 install ray[tune]==2.6.3\n",
    "!pip3 install --upgrade accelerate -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    data_path = \"/content/drive/My Drive/TurkuNLP/rl/data/\"\n",
    "    output_path = \"/content/drive/My Drive/TurkuNLP/rl/output/\"\n",
    "\n",
    "except:\n",
    "    data_path = 'data/'\n",
    "    output_path = 'output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "\n",
    "evaluations = {\n",
    "    \"xmlr-base-fr\": {\n",
    "        #\"model_name\": f\"{output_path}checkpoints/checkpoint-762\",\n",
    "        \"model_name\":\"xlm-roberta-base\",\n",
    "        \"train\": \"fr\",\n",
    "        \"test\": \"fr\",\n",
    "        \"columns\": [\"a\", \"b\", \"label\", \"text\", \"c\"],\n",
    "        \"class_weights\": False,\n",
    "        \"lr\": 3.2708e-05,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"eval_batch_size\": 32,\n",
    "        \"weight_decay\": 0,\n",
    "        \"epochs\": 20,\n",
    "        \"patience\": 5,\n",
    "        \"threshold\": None,\n",
    "        \"cache_dir\": f\"{output_path}cache\",\n",
    "        \"checkpoint_dir\": f\"{output_path}checkpoints\",\n",
    "        \"output_dir\": f\"{output_path}model\",\n",
    "        \"tune_hyperparameters\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# only train and test for these languages\n",
    "small_languages = [\n",
    "    \"ar\",\n",
    "    \"ca\",\n",
    "    \"es\",\n",
    "    \"fa\",\n",
    "    \"hi\",\n",
    "    \"id\",\n",
    "    \"jp\",\n",
    "    \"no\",\n",
    "    \"pt\",\n",
    "    \"tr\",\n",
    "    \"ur\",\n",
    "    \"zh\",\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"HI\",\n",
    "    \"ID\",\n",
    "    \"IN\",\n",
    "    \"IP\",\n",
    "    \"LY\",\n",
    "    \"MT\",\n",
    "    \"NA\",\n",
    "    \"OP\",\n",
    "    \"SP\",\n",
    "    \"av\",\n",
    "    \"ds\",\n",
    "    \"dtp\",\n",
    "    \"ed\",\n",
    "    \"en\",\n",
    "    \"fi\",\n",
    "    \"it\",\n",
    "    \"lt\",\n",
    "    \"nb\",\n",
    "    \"ne\",\n",
    "    \"ob\",\n",
    "    \"ra\",\n",
    "    \"re\",\n",
    "    \"rs\",\n",
    "    \"rv\",\n",
    "    \"sr\",\n",
    "]\n",
    "\n",
    "sub_register_map = {\n",
    "    \"NA\": \"NA\",\n",
    "    \"NE\": \"ne\",\n",
    "    \"SR\": \"sr\",\n",
    "    \"PB\": \"nb\",\n",
    "    \"HA\": \"NA\",\n",
    "    \"FC\": \"NA\",\n",
    "    \"TB\": \"nb\",\n",
    "    \"CB\": \"nb\",\n",
    "    \"OA\": \"NA\",\n",
    "    \"OP\": \"OP\",\n",
    "    \"OB\": \"ob\",\n",
    "    \"RV\": \"rv\",\n",
    "    \"RS\": \"rs\",\n",
    "    \"AV\": \"av\",\n",
    "    \"IN\": \"IN\",\n",
    "    \"JD\": \"IN\",\n",
    "    \"FA\": \"fi\",\n",
    "    \"DT\": \"dtp\",\n",
    "    \"IB\": \"IN\",\n",
    "    \"DP\": \"dtp\",\n",
    "    \"RA\": \"ra\",\n",
    "    \"LT\": \"lt\",\n",
    "    \"CM\": \"IN\",\n",
    "    \"EN\": \"en\",\n",
    "    \"RP\": \"IN\",\n",
    "    \"ID\": \"ID\",\n",
    "    \"DF\": \"ID\",\n",
    "    \"QA\": \"ID\",\n",
    "    \"HI\": \"HI\",\n",
    "    \"RE\": \"re\",\n",
    "    \"IP\": \"IP\",\n",
    "    \"DS\": \"ds\",\n",
    "    \"EB\": \"ed\",\n",
    "    \"ED\": \"ed\",\n",
    "    \"LY\": \"LY\",\n",
    "    \"PO\": \"LY\",\n",
    "    \"SO\": \"LY\",\n",
    "    \"SP\": \"SP\",\n",
    "    \"IT\": \"it\",\n",
    "    \"FS\": \"SP\",\n",
    "    \"TV\": \"SP\",\n",
    "    \"OS\": \"OS\",\n",
    "    \"IG\": \"IP\",\n",
    "    \"MT\": \"MT\",\n",
    "    \"HT\": \"HI\",\n",
    "    \"FI\": \"fi\",\n",
    "    \"OI\": \"IN\",\n",
    "    \"TR\": \"IN\",\n",
    "    \"AD\": \"OP\",\n",
    "    \"LE\": \"OP\",\n",
    "    \"OO\": \"OP\",\n",
    "    \"MA\": \"NA\",\n",
    "    \"ON\": \"NA\",\n",
    "    \"SS\": \"NA\",\n",
    "    \"OE\": \"IP\",\n",
    "    \"PA\": \"IP\",\n",
    "    \"OF\": \"ID\",\n",
    "    \"RR\": \"ID\",\n",
    "    \"FH\": \"HI\",\n",
    "    \"OH\": \"HI\",\n",
    "    \"TS\": \"HI\",\n",
    "    \"OL\": \"LY\",\n",
    "    \"PR\": \"LY\",\n",
    "    \"SL\": \"LY\",\n",
    "    \"TA\": \"SP\",\n",
    "    \"OTHER\": \"OS\",\n",
    "    \"\": \"\",\n",
    "}\n",
    "\n",
    "def get_data(evaluation):\n",
    "    data_files = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "\n",
    "    for l in evaluation[\"train\"].split(\"-\"):\n",
    "        data_files[\"train\"].append(f\"{data_path}{l}/train.tsv\")\n",
    "        if not (l in small_languages):\n",
    "            data_files[\"dev\"].append(f\"{data_path}{l}/dev.tsv\")\n",
    "        else:\n",
    "            # Small languages use test as dev\n",
    "            data_files[\"dev\"].append(f\"{data_path}{l}/test.tsv\")\n",
    "\n",
    "    for l in evaluation[\"test\"].split(\"-\"):\n",
    "        # check if zero-shot for small languages, if yes then test with full data\n",
    "        if l in small_languages and not (l in evaluation[\"train\"].split(\"-\")):\n",
    "            data_files[\"test\"].append(f\"{data_path}{l}/{l}.tsv\")\n",
    "        else:\n",
    "            data_files[\"test\"].append(f\"{data_path}{l}/test.tsv\")\n",
    "\n",
    "    return data_files\n",
    "\n",
    "\n",
    "def compute_class_weights(dataset):\n",
    "    freqs = [0] * len(labels)\n",
    "    n_examples = len(dataset[\"train\"])\n",
    "\n",
    "    for e in dataset[\"train\"][\"label\"]:\n",
    "        for i in range(len(labels)):\n",
    "            if e[i] != 0:\n",
    "                freqs[i] += 1\n",
    "    weights = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        try:\n",
    "            weights.append(n_examples / (len(labels) * freqs[i]))\n",
    "        except:\n",
    "            weights.append(0.0)\n",
    "    print(\"weights:\", weights)\n",
    "    class_weights = torch.FloatTensor(weights)\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def get_class_frequencies(dataset):\n",
    "    y = [0] * len(labels)\n",
    "\n",
    "    for example in dataset[\"train\"]:\n",
    "        for i, val in enumerate(example[\"label\"]):\n",
    "            y[i] += int(val.item())\n",
    "\n",
    "    expanded_y = [index for index, count in enumerate(y) for _ in range(count)]\n",
    "\n",
    "    return expanded_y\n",
    "\n",
    "\n",
    "def optimize_threshold(predictions, labels):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    best_f1 = 0\n",
    "    best_f1_threshold = 0.5  # use 0.5 as a default threshold\n",
    "    y_true = labels\n",
    "    for th in np.arange(0.3, 0.7, 0.05):\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        y_pred[np.where(probs >= th)] = 1\n",
    "        f1 = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_f1_threshold = th\n",
    "    return best_f1_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files: {'train': ['data/data/fr/train.tsv'], 'dev': ['data/data/fr/dev.tsv'], 'test': ['data/data/fr/test.tsv']}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/data/data/fr/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Prepare dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     column_names\u001b[39m=\u001b[39;49mevaluation[\u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     features\u001b[39m=\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mFeatures(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         {x: datasets\u001b[39m.\u001b[39;49mValue(\u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m evaluation[\u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m]}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mevaluation[\u001b[39m\"\u001b[39;49m\u001b[39mcache_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     on_bad_lines\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mskip\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/run.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(evaluation[\u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/TurkuNLP/register-labeling-notebook/venv/lib/python3.11/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2130\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   2131\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2135\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2139\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2141\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   2142\u001b[0m )\n\u001b[1;32m   2144\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/Documents/GitHub/TurkuNLP/register-labeling-notebook/venv/lib/python3.11/site-packages/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1816\u001b[0m     path,\n\u001b[1;32m   1817\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1818\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1819\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1820\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1821\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1822\u001b[0m )\n\u001b[1;32m   1823\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/Documents/GitHub/TurkuNLP/register-labeling-notebook/venv/lib/python3.11/site-packages/datasets/load.py:1430\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[39m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[39m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \n\u001b[1;32m   1422\u001b[0m \u001b[39m# Try packaged\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1424\u001b[0m     \u001b[39mreturn\u001b[39;00m PackagedDatasetModuleFactory(\n\u001b[1;32m   1425\u001b[0m         path,\n\u001b[1;32m   1426\u001b[0m         data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1427\u001b[0m         data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1428\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1429\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m-> 1430\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1431\u001b[0m \u001b[39m# Try locally\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[39melif\u001b[39;00m path\u001b[39m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m~/Documents/GitHub/TurkuNLP/register-labeling-notebook/venv/lib/python3.11/site-packages/datasets/load.py:958\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m base_path \u001b[39m=\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mexpanduser()\u001b[39m.\u001b[39mresolve()\u001b[39m.\u001b[39mas_posix()\n\u001b[1;32m    957\u001b[0m patterns \u001b[39m=\u001b[39m sanitize_patterns(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m get_data_patterns(base_path)\n\u001b[0;32m--> 958\u001b[0m data_files \u001b[39m=\u001b[39m DataFilesDict\u001b[39m.\u001b[39;49mfrom_patterns(\n\u001b[1;32m    959\u001b[0m     patterns,\n\u001b[1;32m    960\u001b[0m     download_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_config,\n\u001b[1;32m    961\u001b[0m     base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m    962\u001b[0m )\n\u001b[1;32m    963\u001b[0m supports_metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[1;32m    964\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m supports_metadata \u001b[39mand\u001b[39;00m patterns \u001b[39m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m~/Documents/GitHub/TurkuNLP/register-labeling-notebook/venv/lib/python3.11/site-packages/datasets/data_files.py:686\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    683\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m()\n\u001b[1;32m    684\u001b[0m \u001b[39mfor\u001b[39;00m key, patterns_for_key \u001b[39min\u001b[39;00m patterns\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    685\u001b[0m     out[key] \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 686\u001b[0m         DataFilesList\u001b[39m.\u001b[39;49mfrom_patterns(\n\u001b[1;32m    687\u001b[0m             patterns_for_key,\n\u001b[1;32m    688\u001b[0m             base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m    689\u001b[0m             allowed_extensions\u001b[39m=\u001b[39;49mallowed_extensions,\n\u001b[1;32m    690\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    691\u001b[0m         )\n\u001b[1;32m    692\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m    693\u001b[0m         \u001b[39melse\u001b[39;00m patterns_for_key\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/GitHub/TurkuNLP/register-labeling-notebook/venv/lib/python3.11/site-packages/datasets/data_files.py:591\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[39mfor\u001b[39;00m pattern \u001b[39min\u001b[39;00m patterns:\n\u001b[1;32m    589\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         data_files\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 591\u001b[0m             resolve_pattern(\n\u001b[1;32m    592\u001b[0m                 pattern,\n\u001b[1;32m    593\u001b[0m                 base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m    594\u001b[0m                 allowed_extensions\u001b[39m=\u001b[39;49mallowed_extensions,\n\u001b[1;32m    595\u001b[0m                 download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    596\u001b[0m             )\n\u001b[1;32m    597\u001b[0m         )\n\u001b[1;32m    598\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/Documents/GitHub/TurkuNLP/register-labeling-notebook/venv/lib/python3.11/site-packages/datasets/data_files.py:380\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[39mif\u001b[39;00m allowed_extensions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m with any supported extension \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(allowed_extensions)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 380\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    381\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/Users/erikhenriksson/Documents/GitHub/TurkuNLP/register-labeling-notebook/data/data/fr/train.tsv'"
     ]
    }
   ],
   "source": [
    "evaluation_name = \"xmlr-base-fr\"\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "pprint = PrettyPrinter(compact=True).pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Init data\n",
    "\n",
    "evaluation = evaluations[evaluation_name]\n",
    "data_files = get_data(evaluation)\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "print(f\"Data files: {data_files}\")\n",
    "\n",
    "# Prepare dataset\n",
    "\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=evaluation[\"columns\"],\n",
    "    features=datasets.Features(\n",
    "        {x: datasets.Value(\"string\") for x in evaluation[\"columns\"]}\n",
    "    ),\n",
    "    cache_dir=evaluation[\"cache_dir\"],\n",
    "    on_bad_lines=\"skip\",\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(evaluation[\"model_name\"])\n",
    "\n",
    "\n",
    "def preprocess_data(example):\n",
    "    text = example[\"text\"] or \"\"\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    mapped_labels = set(\n",
    "        [\n",
    "            sub_register_map[l] if l not in labels else l\n",
    "            for l in (example[\"label\"] or \"NA\").split()\n",
    "        ]\n",
    "    )\n",
    "    encoding[\"label\"] = np.array([1.0 if l in mapped_labels else 0.0 for l in labels])\n",
    "    return encoding\n",
    "\n",
    "\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"a\", \"b\", \"c\"])\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "\n",
    "if evaluation[\"class_weights\"] == True:\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    print(list(range(0, len(labels))))\n",
    "    print(get_class_frequencies(dataset))\n",
    "    class_weights = compute_class_weight(\n",
    "        \"balanced\",\n",
    "        classes=list(range(0, len(labels))),\n",
    "        y=get_class_frequencies(dataset),\n",
    "    )\n",
    "\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        if evaluation[\"class_weights\"] == True:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        else:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def model_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        evaluation[\"model_name\"],\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    evaluation[\"checkpoint_dir\"],\n",
    "    learning_rate=evaluation[\"lr\"],\n",
    "    per_device_train_batch_size=evaluation[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=evaluation[\"eval_batch_size\"],\n",
    "    num_train_epochs=evaluation[\"epochs\"],\n",
    "    weight_decay=evaluation[\"weight_decay\"],\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_steps=100,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1  # configured threshold\n",
    "    y_pred_th05 = np.zeros(probs.shape)\n",
    "    y_pred_th05[np.where(probs >= 0.5)] = 1  # default threshold\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "    f1_micro_average_th05 = f1_score(y_true=y_true, y_pred=y_pred_th05, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {\n",
    "        \"f1\": f1_micro_average,\n",
    "        \"f1_th0.5\": f1_micro_average_th05,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    threshold = (\n",
    "        evaluation[\"threshold\"]\n",
    "        if evaluation[\"threshold\"]\n",
    "        else optimize_threshold(preds, p.label_ids)\n",
    "    )\n",
    "\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, labels=p.label_ids, threshold=threshold\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "trainer = MultiLabelTrainer(\n",
    "    model=None,\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Tune hyperparameters or just train\n",
    "\n",
    "if evaluation[\"tune_hyperparameters\"]:\n",
    "    from ray import tune\n",
    "\n",
    "    asha_scheduler = tune.schedulers.ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    tune_config = {\n",
    "        \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
    "        # \"weight_decay\": tune.choice([0.0, 0.1, 0.2, 0.3]),\n",
    "        \"num_train_epochs\": tune.choice([20]),\n",
    "        \"per_device_train_batch_size\": tune.choice([8, 10]),\n",
    "    }\n",
    "\n",
    "    trainer.hyperparameter_search(\n",
    "        hp_space=lambda _: tune_config, backend=\"ray\", scheduler=asha_scheduler\n",
    "    )\n",
    "\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "print(\"Evaluating with test set...\")\n",
    "eval_results = trainer.evaluate(dataset[\"test\"])\n",
    "\n",
    "pprint(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
